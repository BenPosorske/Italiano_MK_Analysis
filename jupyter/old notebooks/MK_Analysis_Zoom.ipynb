{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MK_Analysis\n",
    "\n",
    "This jupyter notebook is designed to analyze proplatelet production of Megakaryocytes from tiff time-lapses taken by the Incucyte Zoom (10X, 1390x1040). \n",
    "\n",
    "The Analysis workflow occurs in 2 steps:\n",
    "\n",
    "1 ilastikProcessing - Unpacks tiff stacks & generates binary probabilities from phase images through the ilastik project (.ilp) file.\n",
    "\n",
    "\n",
    "2 Production Pipeline - Primary pipeline for proplatelet production analysis. Important output files are listed as follows:\n",
    "    \n",
    "    a. 'results_Image.csv' -> meg & proplatelet count\n",
    "    \n",
    "    b. 'results_cell/pplt.properties' -> use in CPA to train classifiers\n",
    "    \n",
    "    c. 'labels' of proplatelet objects used for the Skeleton pipeline\n",
    "    \n",
    "    d. 'overlay' of phase images, labeling megs as red & pplts as green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ilastikProcessing\n",
    "To effectively use ilastik some formatting must be done before and after ilastik processes the images. The input is assumed to be a time-series of images stored in a multi-page TIFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakrith\\AppData\\Local\\conda\\conda\\envs\\bioimg\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib\n",
    "import numpy\n",
    "import os\n",
    "import os.path\n",
    "import pandas\n",
    "import re\n",
    "import skimage\n",
    "import skimage.exposure\n",
    "import skimage.io\n",
    "import subprocess\n",
    "from shutil import copy2\n",
    "from tqdm._tqdm_notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Unpack (or separate) input images into single files.\n",
    "\n",
    "### Update input image variables\n",
    "Update the variable *image_directory* with the path to a folder that contains the input images. Update the *regex_image* variable to process only the images that match the regular expression. If the *regex_image* variable is equal to `(.*)\\.tif`, then any *.tif* in the folder will be processed.\n",
    "\n",
    "If a filename matches the *regex_single* regular expression, then it is assumed that this image has already been unpacked. An unpacked single image will have timepoint appended to the end of the file following the pattern `\\-\\d{4}\\.tif`\n",
    "\n",
    "*path_to_ilastik* is a string with the path to the ilastik software for [running headless](http://ilastik.org/documentation/basics/headless.html).\n",
    "\n",
    "### **UPDATE IMAGE DIRECTORY BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = r\"C:\\Users\\Prakrith\\Desktop\\ASO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdir_single = r\"C:\\Users\\Prakrith\\Desktop\\ASO\\single_images\"\n",
    "imdir_ilastik = r\"C:\\Users\\Prakrith\\Desktop\\ASO\\ilastik\"\n",
    "output_directory = r\"C:\\Users\\Prakrith\\Desktop\\ASO\\output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ilastik = r\"C:\\Program Files\\ilastik-1.3.0b4\\run-ilastik.bat\"\n",
    "path_to_project = r\"C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\ilps\\180421_Zoom.ilp\"\n",
    "path_to_cellprofiler = r\"C:\\Program Files (x86)\\CellProfiler\\CellProfiler.exe\"\n",
    "path_to_cp_pipeline = r\"C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\pipelines\\180430_MK_Zoom.cppipe\"\n",
    "regex_image = \"(.*)\\.tif\" #stack\n",
    "regex_single = \".*\\-\\d{4}\\.tif\" #slice\n",
    "re_image = re.compile(regex_image)\n",
    "re_single = re.compile(regex_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(image_directory, \"single_images\"), exist_ok=True) \n",
    "imdir_single = os.path.join(image_directory,\"single_images\")\n",
    "os.makedirs(os.path.join(image_directory, \"ilastik\"), exist_ok=True)\n",
    "imdir_ilastik = os.path.join(image_directory,\"ilastik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(image_directory, \"output\"), exist_ok=True)\n",
    "output_directory = os.path.join(image_directory,\"output\")\n",
    "# os.makedirs(os.path.join(image_directory, \"skeleton\"), exist_ok=True)\n",
    "# skeleton_directory = os.path.join(image_directory,\"skeleton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to import image metadata\n",
    "* *is_my_file* will use the regular expression to filter image files to be processed.\n",
    "* *make_dict* parses a file to be processed and places metadata into a dictionary.\n",
    "\n",
    "Parse the files to be processed and then place the metadata into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_my_file(filename, re_image, re_single):\n",
    "    \n",
    "    mybool = False\n",
    "    \n",
    "    if (    re_image.match(filename) != None \n",
    "        and re_single.match(filename) == None\n",
    "       ):\n",
    "        \n",
    "        mybool = True\n",
    "        \n",
    "    return mybool\n",
    "\n",
    "\n",
    "def make_dict(filename, path, re_obj):\n",
    "    \n",
    "    my_dict = re_obj.match(filename).groupdict()\n",
    "    \n",
    "    my_dict[\"filename\"] = filename\n",
    "    \n",
    "    my_dict[\"path\"] = path\n",
    "    \n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files_dict = [make_dict(f, image_directory, re_image) for f in os.listdir(image_directory) if is_my_file(f, re_image, re_single)]\n",
    "image_df = pandas.DataFrame(image_files_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack the multi-page TIFF images\n",
    "For every image, create a single image file for each timepoint. The input images are assumed to be 8-bit, which has 2 dimensions (length, width). The multipage TIFF of 8-bit images will have 3 dimensions (timepoints, width, length). \n",
    "\n",
    "*If the upstream workflow changes and the input image format is altered, then the conditional logic below will need to be updated, specifically the logic based on the shape of the input images.*\n",
    "\n",
    "### Are the images across experiments similar enough to treat equally\n",
    "One concern is that overfitting from training a classification model either through ilastik or CellProfiler analyst. The training set needs to be representative of the possibility space. This is accomplished by choosing a large enough image set that includes images of all states of interest including undifferentiated and fully differentiated megakaryocytes.\n",
    "\n",
    "We also want to eliminate noise from known sources of variablity that could potentially weaken the classifier. The primary sources of noise in the images will be non-uniform illumination and differences in exposure. Non-uniform illumination is difficult to correct, because the background is actually in the middle of the intensity range and the signal occupies both high and low intensities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_stack_image(p):\n",
    "    \n",
    "    im = skimage.io.imread(os.path.join(p[\"path\"], p[\"filename\"]))\n",
    "\n",
    "    number_of_timepoints = im.shape[0]\n",
    "\n",
    "    for i in range(number_of_timepoints):\n",
    "        \n",
    "        retest = re_image.match(p[\"filename\"])\n",
    "\n",
    "        retest.group(1)\n",
    "\n",
    "        fname = \"{0}-{1:04d}.tif\".format(retest.group(1), i)\n",
    "\n",
    "        skimage.io.imsave(os.path.join(p[\"path\"], \"single_images\", fname), im[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7b9ac54bb54b63a170b01741a8b576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='unpack', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if image_df.empty is False:\n",
    "    \n",
    "    # Note that this can fail if the input images aren't in the expected format\n",
    "    # If you receive an error, double check the format of the input images, e.g. are they RGB?\n",
    "    tqdm_notebook.pandas(desc=\"unpack\")\n",
    "    _ = image_df.progress_apply(df_stack_image, axis=1)\n",
    "\n",
    "else:\n",
    "    \n",
    "    print(\"no images to unpack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run ilastik\n",
    "\n",
    "Using the single images created earlier, process the images using ilastik. First, create another dataframe with the single image metadata. Note, this has been written for running on Windows.\n",
    "\n",
    "## Process ilastik output for CellProfiler\n",
    "ilastik will output and HDF5 file that must be parsed for use as input to CellProfiler. This workflow assumes the default export settings are being used in ilastik. We have observed performance costs when changing the exporting settings to formats beyond the standard ilastik HDF5 file. For example, exporting TIFF images changes the shape of the exported data from yxc (the default) to cyx. This rearrangement will cause downstream errors, because the code as written expects the channel to be the third dimension.\n",
    "\n",
    "### ilastik stage-1 labels\n",
    "The project file *Mouse_MK_Training_180101_v1.ilp* has the following labels that are stored in the same order within the HDF5 output.\n",
    "1. background\n",
    "2. border_white\n",
    "3. cell\n",
    "4. protrusion\n",
    "5. background_border\n",
    "6. not_cell\n",
    "\n",
    "### ilastik stage-2 labels\n",
    "The probability images output from the second-stage classification are stored in the ilastik folder as pngs, in the following order:\n",
    "0. background\n",
    "1. cell_boundary\n",
    "2. cell\n",
    "3. protrusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_my_file(filename, re_obj):\n",
    "    \n",
    "    mybool = False\n",
    "    \n",
    "    if re_obj.match(filename) != None:\n",
    "        \n",
    "        mybool = True\n",
    "        \n",
    "    return mybool\n",
    "\n",
    "\n",
    "def make_dict(filename, path, re_obj):\n",
    "    \n",
    "    my_dict = re_obj.match(filename).groupdict()\n",
    "    \n",
    "    my_dict[\"filename\"] = filename\n",
    "    \n",
    "    my_dict[\"path\"] = path\n",
    "    \n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files_dict = [make_dict(f, imdir_single, re_single) for f in os.listdir(imdir_single) if is_my_file(f, re_single)]\n",
    "image_df = pandas.DataFrame(image_files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_ilastik(p):\n",
    "    \n",
    "    filename = os.path.join(p[\"path\"], p[\"filename\"])\n",
    "    \n",
    "    filename_noext = os.path.splitext(p[\"filename\"])[0]\n",
    "    \n",
    "    filename_h5 = \"{}_Probabilities Stage 2.h5\".format(filename_noext)\n",
    "    \n",
    "    # Run ilastik using subprocess\n",
    "    \n",
    "    process = subprocess.Popen([path_to_ilastik, \n",
    "                  \"--headless\",\n",
    "                  \"--export_source=probabilities stage 2\",\n",
    "                  \"--output_format=hdf5\",\n",
    "                  r\"--project={}\".format(path_to_project),\n",
    "                  filename\n",
    "                 ], stdout=subprocess.PIPE)\n",
    "    \n",
    "    out, err = process.communicate()\n",
    "    \n",
    "    # unpack the HDF5 file\n",
    "    \n",
    "    label_list = [\"background\", \"protrusion\", \"cell_boundary\", \"cell\"]\n",
    "    \n",
    "    path_h5 = os.path.join(p[\"path\"], filename_h5)\n",
    "    \n",
    "    with h5py.File(path_h5, \"r\") as ilastik_hdf5:\n",
    "    \n",
    "        ilastik_probabilities = ilastik_hdf5[\"exported_data\"].value\n",
    "    \n",
    "        for i in range(ilastik_probabilities.shape[2]):\n",
    "            im = skimage.img_as_uint(ilastik_probabilities[:, :, i])\n",
    "        \n",
    "            filename_slice = \"{}_{}_prbstg2_{}.png\".format(filename_noext, label_list[i], i)\n",
    "        \n",
    "            skimage.io.imsave(os.path.join(p[\"path\"], \"..\", \"ilastik\", filename_slice), im)\n",
    "    \n",
    "    os.remove(path_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19754aecbbec4362b4c9ede514e49ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='run ilastik', max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakrith\\AppData\\Local\\conda\\conda\\envs\\bioimg\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float32 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm_notebook.pandas(desc=\"run ilastik\")\n",
    "_ = image_df.progress_apply(df_ilastik, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CellProfiler\n",
    "\n",
    "## Make a filelist\n",
    "Add the paths to each file that will be processed by CellProfiler into a text file.\n",
    "\n",
    "## Run CellProfiler Pipeline\n",
    "Use subprocess to run CellProfiler on the images to be processed.\n",
    "\n",
    "Note, that a model that filters protrusions was trained in CellProfiler Analyst outside of this workflow. The model has to be in the input folder to be found by CellProfiler. (\"CPTemp_in\" folder on Desktop, fgb_rules_pplt.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Prakrith\\\\Desktop\\\\ASO\\\\fgb_rules_pplt.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CPA_Rules = r'C:\\Users\\Prakrith\\Desktop\\CPTemp_in\\fgb_rules_pplt.txt'\n",
    "copy2(CPA_Rules, image_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_list = glob.glob(os.path.join(imdir_single,\"*.tif\"))\n",
    "ilastik_list = glob.glob(os.path.join(imdir_ilastik,\"*.png\"))\n",
    "big_list = single_list + ilastik_list\n",
    "with open(os.path.join(image_directory,\"filelist.txt\"), 'w') as f:\n",
    "    for item in big_list:\n",
    "        f.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen([path_to_cellprofiler,\n",
    "                  \"--run-headless\",\n",
    "                  \"--pipeline={}\".format(path_to_cp_pipeline),\n",
    "                  \"--file-list={}\".format(os.path.join(image_directory,\"filelist.txt\")),\n",
    "                  \"--image-directory={}\".format(image_directory),\n",
    "                  \"--output-directory={}\".format(output_directory)\n",
    "                 ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "out, err = process.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify Proplatelet Production\n",
    "From the csvs generated by CellProfiler, the file 'results_Image' is parsed, & proplatelet production is quantified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = open(os.path.join(output_directory,r'results_Image.csv'));\n",
    "header = [\"URL_phase\",\"Count_proplatelets\",\"Count_megs\",\"ImageNumber\"]; \n",
    "df = pandas.read_csv(result, usecols = header, index_col = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (((x[1])/(x[0] + x[1]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(output_directory,r'Raw_Pct.csv'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.apply(f,axis=1);\n",
    "df['normalized_pct'] = x;\n",
    "df = df.set_index(\"ImageNumber\");\n",
    "df.to_csv(os.path.join(output_directory,r'Raw_Pct.csv'));\n",
    "df2 = pandas.DataFrame();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 23; #change n to equivalent timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(x),n):\n",
    "    slc = x.iloc[i:i+n];\n",
    "    slc = slc.reset_index(drop=True);\n",
    "    df2 = pandas.concat([df2,slc],axis=1,ignore_index=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(os.path.join(output_directory,r'Graph_Pct.csv'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
