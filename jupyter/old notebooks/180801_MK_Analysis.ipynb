{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 180801_MK_Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakrith\\AppData\\Local\\conda\\conda\\envs\\bioimg\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import h5py\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import re\n",
    "from shutil import copy2, move\n",
    "import skimage\n",
    "import skimage.exposure\n",
    "import skimage.io\n",
    "import subprocess\n",
    "from tqdm._tqdm_notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alphanumeric sorter --> keep everything organized\n",
    "def sorted_nicely(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Project \n",
    "\n",
    "image_directory = r\"C:\\Users\\Prakrith\\Dropbox (Partners HealthCare)\\WORK\\SHARED\\ARW\\180717_SP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Application paths for subprocess\n",
    "\n",
    "path_to_ilastik = r\"C:\\Program Files\\ilastik-1.3.0b4\\run-ilastik.bat\"\n",
    "\n",
    "path_to_cellprofiler = r\"C:\\Program Files (x86)\\CellProfiler\\CellProfiler.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **CPA Rules** - Cell Profiler Analyst Rules \n",
    "2) **project** means ilastik project file (ilp) \n",
    "3) **cp_pipeline** refers to CellProfiler pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Human pipeline variables\n",
    "\n",
    "CPA_Rules_human = r'C:\\Users\\Prakrith\\Desktop\\CPTemp_in\\fgb_rules_human.txt'\n",
    "\n",
    "path_to_project_human = r\"C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\ilps\\180805_HumanMK.ilp\"\n",
    "\n",
    "path_to_cp_pipeline_human = r\"C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\pipelines\\180527_HP.cppipe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Murine pipeline variables\n",
    "\n",
    "CPA_Rules_mice = r'C:\\Users\\Prakrith\\Desktop\\CPTemp_in\\fgb_rules_pplt.txt'\n",
    "\n",
    "path_to_project_mice = r\"C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\ilps\\180805_FLMK.ilp\"\n",
    "\n",
    "path_to_cp_pipeline_mice2 = r\"C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\pipelines\\180804_MPFluo.cppipe\"\n",
    "\n",
    "path_to_cp_pipeline_mice1 = r\"C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\pipelines\\180804_MP.cppipe\"\n",
    "\n",
    "path_to_skel_pipeline_mice = r\"C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\pipelines\\Kyle_Skel.cppipe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fluorescence pipeline variables\n",
    "\n",
    "path_to_coloc_pipeline =  r'C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\pipelines\\180712_TMK.cppipe'\n",
    "\n",
    "path_to_rfp_pipeline = r'C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\pipelines\\180728_RFP.cppipe'\n",
    "\n",
    "path_to_gfp_pipeline = r'C:\\Users\\Prakrith\\Documents\\GitHub\\Test\\pipelines\\180728_GFP.cppipe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image_directory = r\"/home/prakrith/Applications/test2/\"\n",
    "# path_to_ilastik = r\"/home/prakrith/Applications/ilastik-1.3.0-Linux/run_ilastik.sh\"\n",
    "# path_to_project = r\"/home/prakrith/Applications/ilastik-1.3.0-Linux/180517_Zoom.ilp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpack (or separate) input images into single files.\n",
    "\n",
    "To effectively use ilastik some formating must be done before and after ilastik processes the images. The input is assumed to be a time-series of images stored in a multi-page TIFF.\n",
    "\n",
    "## Update input image variables\n",
    "Update the variable *image_directory* with the path to a folder that contains the input images. Update the *regex_image* variable to process only the images that match the regular expression. If the *regex_image* variable is equal to `(.*)\\.tif`, then any *.tif* in the folder will be processed.\n",
    "\n",
    "If a filename matches the *regex_single* regular expression, then it is assumed that this image has already been unpacked. An unpacked single image will have timepoint appended to the end of the file following the pattern `\\-\\d{4}\\.tif`\n",
    "\n",
    "*path_to_ilastik* is a string with the path to the ilastik software for [running headless](http://ilastik.org/documentation/basics/headless.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_image = \"(.*)\\.tif\" #stack\n",
    "regex_single = \".*\\-\\d{4}\\.tif\" #slice\n",
    "re_image = re.compile(regex_image)\n",
    "re_single = re.compile(regex_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(image_directory, \"single_images\"), exist_ok=True)\n",
    "imdir_single = os.path.join(image_directory,\"single_images\")\n",
    "os.makedirs(os.path.join(image_directory, \"ilastik\"), exist_ok=True)\n",
    "imdir_ilastik = os.path.join(image_directory,\"ilastik\")\n",
    "os.makedirs(os.path.join(image_directory, \"output\"), exist_ok=True)\n",
    "output_directory = os.path.join(image_directory,\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image_directory = r\"C:\\Users\\Prakrith\\Desktop\\test\"\n",
    "# imdir_single = r\"C:\\Users\\Prakrith\\Desktop\\test\\single_images\"\n",
    "# imdir_ilastik = r\"C:\\Users\\Prakrith\\Desktop\\test\\ilastik\"\n",
    "# output_directory = r\"C:\\Users\\Prakrith\\Desktop\\test\\output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to import image metadata\n",
    "* *is_my_file* will use the regular expression to filter image files to be processed.\n",
    "* *make_dict* parses a file to be processed and places metadata into a dictionary.\n",
    "\n",
    "Parse the files to be processed and then place the metadata into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_my_file(filename, re_image, re_single):\n",
    "    \n",
    "    mybool = False\n",
    "    \n",
    "    if (    re_image.match(filename) != None \n",
    "        and re_single.match(filename) == None\n",
    "       ):\n",
    "        \n",
    "        mybool = True\n",
    "        \n",
    "    return mybool\n",
    "\n",
    "\n",
    "def make_dict(filename, path, re_obj):\n",
    "    \n",
    "    my_dict = re_obj.match(filename).groupdict()\n",
    "    \n",
    "    my_dict[\"filename\"] = filename\n",
    "    \n",
    "    my_dict[\"path\"] = path\n",
    "    \n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack the multi-page phase TIFF images\n",
    "For every image, create a single image file for each timepoint. The input images are assumed to be RGB, which has 3 dimensions (length, width, color). The multipage TIFF of RGB images will have 4 dimensions (timepoints, length, width, color). \n",
    "\n",
    "*If the upstream workflow changes and the input image format is altered, then the conditional logic below will need to be updated, specifically the logic based on the shape of the input images.*\n",
    "\n",
    "### Are the images across experiments similar enough to treat equally\n",
    "One concern is that overfitting from training a classification model either through ilastik or CellProfiler analyst. The training set needs to be representative of the possibility space. This is accomplished by choosing a large enough image set that includes images of all states of interest including undifferentiated and fully differentiated megakaryocytes.\n",
    "\n",
    "We also want to eliminate noise from known sources of variablity that could potentially weaken the classifier. The primary sources of noise in the images will be non-uniform illumination and differences in exposure. Non-uniform illumination is difficult to correct, because the background is actually in the middle of the intensity range and the signal occupies both high and low intensities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_stack_image(image_df,folder):\n",
    "    \n",
    "    for n in image_df.index:\n",
    "        \n",
    "        im = skimage.io.imread(os.path.join(image_df[\"path\"][n], image_df[\"filename\"][n]))\n",
    "        \n",
    "        if len(im.shape) < 4:\n",
    "            \n",
    "            retest = re_image.match(image_df[\"filename\"][n])\n",
    "            retest.group(1)\n",
    "            fname = \"{0}-{1:04d}.tif\".format(retest.group(1), 0)\n",
    "            im2 = skimage.color.rgb2gray(im)\n",
    "            im2 = skimage.img_as_ubyte(im2)\n",
    "            skimage.io.imsave(os.path.join(image_df[\"path\"][n], fname), im2)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            number_of_timepoints = im.shape[0]\n",
    "            \n",
    "            for i in range(number_of_timepoints):\n",
    "                \n",
    "                retest = re_image.match(image_df[\"filename\"][n])\n",
    "                retest.group(1)\n",
    "                fname = \"{0}-{1:04d}.tif\".format(retest.group(1), i)\n",
    "                im2 = skimage.color.rgb2gray(im[i,:,:,:])\n",
    "                im2 = skimage.img_as_ubyte(im2)\n",
    "                skimage.io.imsave(os.path.join(image_df[\"path\"][n], folder, fname), im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_files_dict = [make_dict(f, image_directory, re_image) for f in os.listdir(image_directory) if is_my_file(f, re_image, re_single)]\n",
    "image_df = pd.DataFrame(image_files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if image_df.empty is False:\n",
    "    \n",
    "    df_stack_image(image_df,\"single_images\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"no phase images to unpack.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluorescence Check\n",
    "\n",
    "Check to see if fluorescence folders (**rfp** or **gfp**) containing fluo stacks exist in the main image directory - if yes Proplatelet Production pipeline will generate MK label images along with the usual proplatelet labels to quantify fluo within both populations. If no fluo folders are detected, only the proplatelet labels will be created for the skel pipe, reducing processing time & project size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.isdir(image_directory + '\\\\' + 'rfp') == True:\n",
    "\n",
    "    for dirpath, dirnames, files in os.walk(image_directory + '\\\\' + 'rfp'):\n",
    "        if files:    \n",
    "            rfp = os.path.join(image_directory,\"rfp\")\n",
    "            os.makedirs(os.path.join(rfp, \"rfp_single\"), exist_ok=True)\n",
    "            rfp_single = os.path.join(rfp, \"rfp_single\")\n",
    "            rfp_files_dict = [make_dict(f, rfp, re_image) for f in os.listdir(rfp) if is_my_file(f, re_image, re_single)]\n",
    "            rfp_df = pd.DataFrame(rfp_files_dict)\n",
    "            \n",
    "            if rfp_df.empty is False:\n",
    "                df_stack_image(rfp_df,\"rfp_single\")\n",
    "                rfp_list = sorted_nicely(glob.glob(os.path.join(rfp_single,\"*.tif\")))\n",
    "                \n",
    "                if len(rfp_list) != len(single_list):\n",
    "                    print(\"number of rfp single_images doesn't match number of phase single_images\")\n",
    "                    break;\n",
    "                else:\n",
    "                    pass;\n",
    "            else:\n",
    "                break;\n",
    "#                 rfp_list = sorted_nicely(glob.glob(os.path.join(gfp_single,\"*.tif\")))\n",
    "#                 print(\"no rfp images to unpack.\")\n",
    "                \n",
    "        if not files:\n",
    "            print(dirpath, 'is empty')\n",
    "            \n",
    "else:\n",
    "    rfp = None;\n",
    "\n",
    "if os.path.isdir(image_directory + '\\\\' + 'gfp') == True:\n",
    "    \n",
    "    for dirpath, dirnames, files in os.walk(image_directory + '\\\\' + 'gfp'):\n",
    "        if files:\n",
    "            \n",
    "            gfp = os.path.join(image_directory,\"gfp\")\n",
    "            os.makedirs(os.path.join(gfp, \"gfp_single\"), exist_ok=True)\n",
    "            gfp_single = os.path.join(gfp, \"gfp_single\")\n",
    "            gfp_files_dict = [make_dict(f, gfp, re_image) for f in os.listdir(gfp) if is_my_file(f, re_image, re_single)]\n",
    "            gfp_df = pd.DataFrame(gfp_files_dict)\n",
    "                    \n",
    "            if gfp_df.empty is False:\n",
    "                \n",
    "                df_stack_image(gfp_df,\"gfp_single\")\n",
    "                gfp_list = sorted_nicely(glob.glob(os.path.join(gfp_single,\"*.tif\")))\n",
    "                \n",
    "                if len(gfp_list) != len(single_list):\n",
    "                    print(\"number of gfp single_images doesn't match number of phase single_images\")\n",
    "                    break;\n",
    "                else:\n",
    "                    pass;          \n",
    "            else:\n",
    "                break;\n",
    "#                 gfp_list = sorted_nicely(glob.glob(os.path.join(gfp_single,\"*.tif\")))\n",
    "#                 print(\"no gfp images to unpack.\")\n",
    "                \n",
    "        if not files:\n",
    "            print(dirpath, 'is empty')\n",
    "    \n",
    "else:\n",
    "    gfp = None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if rfp is not None and gfp is not None:\n",
    "    fluo = 1 #both\n",
    "elif rfp is not None and gfp is None:\n",
    "    fluo = 2 #rfp\n",
    "elif rfp is None and gfp is not None:\n",
    "    fluo = 3 #gfp\n",
    "else:\n",
    "    fluo = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]Human MK or [2]Mice MK ?2\n"
     ]
    }
   ],
   "source": [
    "ans1 = ['1']\n",
    "ans2 = ['2']\n",
    "\n",
    "celltype = str(input(\"[1]Human MK or [2]Mice MK ?\"))\n",
    "\n",
    "if celltype in ans1:\n",
    "    \n",
    "    #directory with location of human MK pplt CellProfiler Analyst Rules\n",
    "    \n",
    "    CPA_Rules = CPA_Rules_human \n",
    "    \n",
    "    #copy rules.txt to project folder\n",
    "    copy2(CPA_Rules, image_directory)\n",
    "    \n",
    "    path_to_project = path_to_project_human\n",
    "    path_to_cp_pipeline = path_to_cp_pipeline_human\n",
    "    \n",
    "elif celltype in ans2:\n",
    "        \n",
    "    CPA_Rules = CPA_Rules_mice\n",
    "    copy2(CPA_Rules, image_directory)\n",
    "    \n",
    "    path_to_project = path_to_project_mice\n",
    "        \n",
    "    if fluo is not False:\n",
    "        # Alternative pipe for generating MK labels for fluo analysis\n",
    "        path_to_cp_pipeline = path_to_cp_pipeline_mice2\n",
    "    else:\n",
    "        # Standard pplt production pipeline\n",
    "        path_to_cp_pipeline = path_to_cp_pipeline_mice1\n",
    "    \n",
    "else:\n",
    "    print(\"Invalid input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scans per well?4\n"
     ]
    }
   ],
   "source": [
    "# Include the number of scans to account for mean/std_dev when generating graphs downstream\n",
    "    \n",
    "ans3 = ['1','2','3','4','9']\n",
    "\n",
    "numscans = input(\"Number of scans per well?\")\n",
    "\n",
    "if numscans in ans3:\n",
    "    numscans = int(numscans)\n",
    "else:\n",
    "    print(\"Invalid number of scans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ilastikProcessing\n",
    "\n",
    "Using the single images created earlier, process the images using ilastik. First, create another dataframe with the single image metadata. Note, this has been written for running on Windows.\n",
    "\n",
    "## Process ilastik output for CellProfiler\n",
    "ilastik will output an HDF5 file that must be parsed for use as input to CellProfiler. This workflow assumes the default export settings are being used in ilastik. We have observed performance costs when changing the exporting settings to formats beyond the standard ilastik HDF5 file. For example, exporting TIFF images changes the shape of the exported data from yxc (the default) to cyx. This rearrangement will cause downstream errors, because the code as written expects the channel to be the third dimension.\n",
    "\n",
    "### ilastik stage-2 labels\n",
    "The ilastik project file(s) (*.ilp*) has the following labels that are stored in the same order within the HDF5 output.\n",
    "1. background\n",
    "1. cell_boundary\n",
    "1. cell\n",
    "1. protrusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_my_file2(filename, re_obj):\n",
    "    \n",
    "    mybool = False\n",
    "    \n",
    "    if re_obj.match(filename) != None:\n",
    "        \n",
    "        mybool = True\n",
    "        \n",
    "    return mybool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_ilastik(p):\n",
    "    \n",
    "    filename = os.path.join(p[\"path\"], p[\"filename\"])\n",
    "    \n",
    "    filename_noext = os.path.splitext(p[\"filename\"])[0]\n",
    "    \n",
    "    filename_h5 = \"{}_Probabilities Stage 2.h5\".format(filename_noext)\n",
    "    \n",
    "    # Run ilastik using subprocess\n",
    "    \n",
    "    command = (path_to_ilastik,\"--headless\",\"--export_source=probabilities stage 2\",\"--output_format=hdf5\",\n",
    "               r\"--project={}\".format(path_to_project),filename)\n",
    "    \n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE)\n",
    "    \n",
    "    out, err = process.communicate()\n",
    "    \n",
    "    # unpack the HDF5 file\n",
    "    \n",
    "    label_list = [\"background\", \"protrusion\", \"cell_boundary\", \"cell\"]\n",
    "    \n",
    "    path_h5 = os.path.join(p[\"path\"], filename_h5)\n",
    "    \n",
    "    with h5py.File(path_h5, \"r\") as ilastik_hdf5:\n",
    "    \n",
    "        ilastik_probabilities = ilastik_hdf5[\"exported_data\"].value\n",
    "    \n",
    "        for i in range(ilastik_probabilities.shape[2]):\n",
    "            im = skimage.img_as_uint(ilastik_probabilities[:, :, i])\n",
    "        \n",
    "            filename_slice = \"{}_{}_prbstg2_{}.png\".format(filename_noext, label_list[i], i)\n",
    "        \n",
    "            skimage.io.imsave(os.path.join(p[\"path\"], \"..\", \"ilastik\", filename_slice), im)\n",
    "    \n",
    "    os.remove(path_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_files_dict = [make_dict(f, imdir_single, re_single) for f in os.listdir(imdir_single) if is_my_file2(f, re_single)]\n",
    "image_df = pd.DataFrame(image_files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tqdm_notebook.pandas(desc=\"run ilastik\")\n",
    "# _ = image_df.progress_apply(df_ilastik, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = image_df.apply(df_ilastik, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After ilastikProcessing on HMS O2, issues have been noticed where probability maps are uncapitalized when exported; CP processing is CASE SENSITIVE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for file in os.listdir(imdir_ilastik):\n",
    "#     first = file.split('-', 1)[0].replace('.', '').upper()\n",
    "#     last = file.split('-', 1)[1]\n",
    "#     newName = str(first + '-' + last)\n",
    "#     os.rename(os.path.join(imdir_ilastik, file), os.path.join(imdir_ilastik, newName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ppltProcessing\n",
    "\n",
    "## Make a filelist\n",
    "Add the paths to each file that will be processed by CellProfiler into a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_list = sorted_nicely(glob.glob(os.path.join(imdir_single,\"*.tif\")))\n",
    "ilastik_list = sorted_nicely(glob.glob(os.path.join(imdir_ilastik,\"*.png\")))\n",
    "big_list = single_list + ilastik_list\n",
    "big_list = sorted_nicely(big_list)\n",
    "with open(os.path.join(image_directory,\"pplt_list.txt\"), 'w') as f:\n",
    "    for item in big_list:\n",
    "        f.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Proplatelet Production Pipeline\n",
    "Use subprocess to run CellProfiler on the images to be processed.\n",
    "\n",
    "Note, that a model that filters protrusions was trained in CellProfiler Analyst outside of this workflow. The model has to be in the input folder to be found by CellProfiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command = (path_to_cellprofiler,\"--run-headless\",\"--pipeline={}\".format(path_to_cp_pipeline),\n",
    "           \"--file-list={}\".format(os.path.join(image_directory,\"pplt_list.txt\")),\n",
    "           \"--image-directory={}\".format(image_directory),\"--output-directory={}\".format(output_directory))\n",
    "\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    \n",
    "out, err = process.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Quantify Proplatelet Production\n",
    "From the csvs generated by CellProfiler, the file 'results_Image' is parsed. Proplatelet production & area (mm^2) are quantified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# move raw \"results_\" files to separate folder\n",
    "os.makedirs(os.path.join(output_directory, \"raw_files\"), exist_ok=True)\n",
    "dst = os.path.join(output_directory, \"raw_files\")\n",
    "\n",
    "for filename in os.listdir(output_directory):\n",
    "    if filename.startswith('results'):\n",
    "        src = output_directory + \"\\\\\" + filename\n",
    "        newdst = dst + '\\\\' + filename\n",
    "        move(src, newdst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = open(os.path.join(dst,r'results_Image.csv')); \n",
    "df = pd.read_csv(result,index_col = \"URL_phase\");\n",
    "df = df.reindex(index=sorted_nicely(df.index));\n",
    "df.drop(df.columns[[4,5,8,10,11,12,13,14,15,16]], axis=1, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ppltpct(pplt,mk):\n",
    "    try:\n",
    "        return ((pplt/(mk+pplt))*100) #(count_proplatelet/(count_meg+count_proplatelet)*100)\n",
    "    except ZeroDivisionError:\n",
    "        pass;\n",
    "    \n",
    "def area_mm(area_px):\n",
    "    return (((area_px * 2159000) / 1447680)/1000000); #[(area * total_area_um)/total_area_px)/1x10^6] -> area mm²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['PpltPct'] = df.apply(lambda row: ppltpct(row['Count_proplatelets'], row['Count_megs']), axis=1)\n",
    "df['Area_MK'] = df.apply(lambda row: area_mm(row['AreaOccupied_AreaOccupied_megs']), axis=1)\n",
    "df['Area_Pplt'] = df.apply(lambda row: area_mm(row['AreaOccupied_AreaOccupied_proplatelets']), axis=1)\n",
    "df['Perimeter_MK'] = df.apply(lambda row: area_mm(row['AreaOccupied_Perimeter_megs']), axis=1)\n",
    "df['Perimeter_Pplt'] = df.apply(lambda row: area_mm(row['AreaOccupied_Perimeter_proplatelets']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(df.columns[[0,1,2,3,4,5]], axis=1, inplace=True);\n",
    "df = df.set_index(\"ImageNumber\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_format(directory,single_list,df):\n",
    "    stack_list=[]\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.tif'):\n",
    "            stack_list.append(filename)\n",
    "        \n",
    "    stack_list = sorted_nicely(stack_list)\n",
    "    t = int(len(single_list)); #total num images\n",
    "    n = int((len(single_list) / len(stack_list))); # slices per stack\n",
    "    \n",
    "    df2 = pd.DataFrame();\n",
    "    for i in range(0,t,n):\n",
    "        slc = df.iloc[i:i+n];\n",
    "        slc = slc.reset_index(drop=True);\n",
    "        df2 = pandas.concat([df2,slc],axis=1,ignore_index=True); #iter df by stack length (n), and concat \n",
    "        \n",
    "    return stack_list,n,df2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df2 returns as a tuple (stack_list,number of timepoints,df2)\n",
    "df2 = df_format(image_directory,single_list,df)\n",
    "phase_stack_list = df2[0]\n",
    "n = df2[1]\n",
    "df2 = df2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_format(df,stack_list,n,output_dir,name): #dataframe,stack_list,num_timepoints,output_directory,csv output name\n",
    "    df.columns = stack_list;\n",
    "    df['Timepoint'] = list(range(1,n+1));\n",
    "    df = df.set_index(\"Timepoint\");\n",
    "    df.to_csv(os.path.join(output_dir,name));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_format(df2.loc[:,::5],phase_stack_list,n,output_directory,r'Pplt_Pct.csv');\n",
    "csv_format(df2.loc[:,1::5],phase_stack_list,n,output_directory,r'Area_MK.csv');\n",
    "csv_format(df2.loc[:,2::5],phase_stack_list,n,output_directory,r'Area_Pplt.csv');\n",
    "csv_format(df2.loc[:,3::5],phase_stack_list,n,output_directory,r'Perimeter_MK.csv');\n",
    "csv_format(df2.loc[:,4::5],phase_stack_list,n,output_directory,r'Perimeter_Pplt.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Graphs\n",
    "\n",
    "- Per well graphs depicting total integrated_intensity of both mk/pplt objects over time are generated (w/ mean & std_dev) automatically. Use the formatted csvs for more specific plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "well_list = sorted_nicely(list(set([s.replace('', '')[:-6] for s in phase_stack_list])))\n",
    "os.makedirs(os.path.join(output_directory, \"graphs\"), exist_ok=True)\n",
    "graph_directory = os.path.join(output_directory, \"graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(df,folder,numscans,well_list,ylabel):\n",
    "    mean_df = df.groupby(np.arange(len(df.columns))//numscans, axis=1).mean()\n",
    "    mean_df.columns = well_list\n",
    "    sd_df = df.groupby(np.arange(len(df.columns))//numscans, axis=1).std()\n",
    "    sd_df.columns = well_list\n",
    "    os.makedirs((os.path.join(graph_directory, folder)), exist_ok=True)\n",
    "    out_dir = os.path.join(graph_directory, folder)\n",
    "    \n",
    "    c = 0\n",
    "    for column in mean_df,sd_df:\n",
    "        while c in range(len(well_list)):\n",
    "            fig1, ax1 = plt.subplots()\n",
    "            ax1.set_title(well_list[c])\n",
    "            ax1.set_xlabel(\"Hour\")\n",
    "            ax1.set_ylabel(ylabel)\n",
    "            mean_df2 = mean_df[well_list[c]]\n",
    "            sd_df2 = sd_df[well_list[c]]\n",
    "            #change the code below, for different graphs\n",
    "            mean_df2.plot.line(yerr=sd_df2)\n",
    "            plt.savefig(out_dir + '\\\\' + well_list[c] + '.png', bbox_inches='tight')\n",
    "            plt.close()\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(df2.loc[:,::5],r'pct_pplt',numscans,well_list,r\"Percent Proplatelet-Producing MKs\")\n",
    "plot(df2.loc[:,1::5],r'area_mk',numscans,well_list,r\"Total MK Area (mm²)\")\n",
    "plot(df2.loc[:,2::5],r'area_pplt',numscans,well_list,r\"Total Pplt Area (mm²)\")\n",
    "plot(df2.loc[:,3::5],r'perimeter_mk',numscans,well_list,r\"Total MK Perimeter Area (mm²)\")\n",
    "plot(df2.loc[:,4::5],r'perimeter_pplt',numscans,well_list,r\"Total Pplt Perimeter Area (mm²)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fluoProcessing\n",
    "- Currently one pipeline exists \"quantifying\" ARWs transduced MKs\n",
    "- **In Progress** - Fully capable of analyzing fluo from the incucyte, but the pipeline is not fleshed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if fluo is not False:\n",
    "    \n",
    "    imdir_mk_label = os.path.join(output_directory,\"mk_labels\");\n",
    "    mk_list = sorted_nicely(glob.glob(os.path.join(imdir_mk_label,\"*.tiff\")));\n",
    "    imdir_pplt_label = os.path.join(output_directory,\"proplatelet_labels\");\n",
    "    pplt_list = sorted_nicely(glob.glob(os.path.join(imdir_pplt_label,\"*.tiff\")));\n",
    "    label_list = mk_list + pplt_list\n",
    "    \n",
    "    if len(mk_list) and len(pplt_list) != len(single_list):\n",
    "        print(\"number of fluo single_images doesn't match number of phase single_images\")\n",
    "        break;\n",
    "    else:\n",
    "        pass;\n",
    "    \n",
    "    if fluo == 1:\n",
    "        \n",
    "        fluo_list = single_list + rfp_list + gfp_list + label_list\n",
    "        fluo_list = sorted_nicely(fluo_list)\n",
    "        \n",
    "        with open(os.path.join(image_directory,\"fluo_list.txt\"), 'w') as f:\n",
    "            for item in fluo_list:\n",
    "                f.write(\"{}\\n\".format(item))\n",
    "        \n",
    "        path_to_fluo_pipeline =  path_to_coloc_pipeline\n",
    "        \n",
    "    elif fluo == 2:\n",
    "        \n",
    "        fluo_list = single_list + rfp_list + label_list\n",
    "        fluo_list = sorted_nicely(fluo_list)\n",
    "        \n",
    "        with open(os.path.join(image_directory,\"fluo_list.txt\"), 'w') as f:\n",
    "            for item in fluo_list:\n",
    "                f.write(\"{}\\n\".format(item))\n",
    "                \n",
    "        path_to_fluo_pipeline = path_to_rfp_pipeline\n",
    "        \n",
    "    elif fluo == 3:\n",
    "        \n",
    "        fluo_list = single_list + gfp_list + label_list\n",
    "        fluo_list = sorted_nicely(fluo_list)\n",
    "    \n",
    "        with open(os.path.join(image_directory,\"fluo_list.txt\"), 'w') as f:\n",
    "            for item in fluo_list:\n",
    "                f.write(\"{}\\n\".format(item))\n",
    "                      \n",
    "        path_to_fluo_pipeline = path_to_gfp_pipeline\n",
    "    \n",
    "    else:\n",
    "        print(\"Processing phase skeleton pipeline...\")       \n",
    "        \n",
    "else:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Fluorescence Pipeline\n",
    "Use subprocess to run CellProfiler on the phase & 16-bit fluo images to be processed. 3 pipes have been created to measure gfp, rfp, & co-stained MKs/Pplts. Fluo pipes have been placed in if/else statements for instances where fluo is not present -> will proceed to phase skeleton pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if fluo is not False:\n",
    "    \n",
    "    os.makedirs(os.path.join(image_directory, \"fluo\"), exist_ok=True)\n",
    "    fluo_directory = os.path.join(image_directory,\"fluo\");\n",
    "else:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if fluo is not False:\n",
    "    \n",
    "    command = (path_to_cellprofiler,\"--run-headless\",\"--pipeline={}\".format(path_to_fluo_pipeline),\n",
    "           \"--file-list={}\".format(os.path.join(image_directory,\"fluo_list.txt\")),\n",
    "           \"--image-directory={}\".format(image_directory),\"--output-directory={}\".format(fluo_directory))\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "    out, err = process.communicate()\n",
    "else:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Quantify MK/Proplatelet Fluorescence\n",
    "From the csvs generated by CellProfiler, the file 'results_Masked[_]MK/Pplt' is parsed. Fluorescent intensities and polarity within MKs are quantified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pct_pos(fluo_obj,total_obj):\n",
    "    try:\n",
    "        return ((fluo_obj/(total_obj))*100)\n",
    "    except ZeroDivisionError:\n",
    "        pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if fluo is not False:\n",
    "    \n",
    "# move raw \"results_\" files to separate folder\n",
    "    os.makedirs(os.path.join(fluo_directory, \"raw_files\"), exist_ok=True)\n",
    "    dst = os.path.join(fluo_directory, \"raw_files\")\n",
    "\n",
    "    for filename in os.listdir(fluo_directory):\n",
    "        if filename.startswith('results'):\n",
    "            src = fluo_directory + \"\\\\\" + filename\n",
    "            newdst = dst + '\\\\' + filename\n",
    "            move(src, newdst)\n",
    "            \n",
    "    if fluo == 1:\n",
    "        pass;\n",
    "    \n",
    "    elif fluo == 2:\n",
    "        \n",
    "        result = open(os.path.join(dst,r'results_Image.csv')); \n",
    "        df = pd.read_csv(result,index_col = \"URL_phase\");\n",
    "        df = df.reindex(index=sorted_nicely(df.index));\n",
    "        df.drop(df.columns[[5,7,8,9,10,11,12,13,14]], axis=1, inplace=True);\n",
    "        \n",
    "        df['Pct_RFP_MK'] = df.apply(lambda row: pct_pos(row['Count_MaskedRedmk'], row['Count_mkobj']), axis=1)\n",
    "        df['Pct_RFP_Pplt'] = df.apply(lambda row: pct_pos(row['Count_MaskedRedpplt'], row['Count_ppltobj']), axis=1)\n",
    "        \n",
    "        df.drop(df.columns[[0,1,2,3,4]], axis=1, inplace=True);\n",
    "        df = df.set_index(\"ImageNumber\");\n",
    "        \n",
    "        df2 = df_format(rfp,rfp_list,df)\n",
    "        rfp_stack_list = df2[0]\n",
    "        n = df2[1]\n",
    "        df2 = df2[2]\n",
    "        \n",
    "        csv_format(df2.loc[:,::2],rfp_stack_list,n,fluo_directory,r'PctPositive_RFP_MK.csv');\n",
    "        csv_format(df2.loc[:,1::2],rfp_stack_list,n,fluo_directory,r'PctPositive_RFP_Pplt.csv');\n",
    "        \n",
    "    elif fluo == 3:\n",
    "        \n",
    "        result = open(os.path.join(dst,r'results_Image.csv')); \n",
    "        df = pd.read_csv(result,index_col = \"URL_phase\");\n",
    "        df = df.reindex(index=sorted_nicely(df.index));\n",
    "        df.drop(df.columns[[5,7,8,9,10,11,12,13,14]], axis=1, inplace=True);\n",
    "        \n",
    "        df['Pct_GFP_MK'] = df.apply(lambda row: pct_pos(row['Count_MaskedGreenmk'], row['Count_mkobj']), axis=1)\n",
    "        df['Pct_GFP_Pplt'] = df.apply(lambda row: pct_pos(row['Count_MaskedGreenpplt'], row['Count_ppltobj']), axis=1)\n",
    "        \n",
    "        df.drop(df.columns[[0,1,2,3,4]], axis=1, inplace=True);\n",
    "        df = df.set_index(\"ImageNumber\");\n",
    "        \n",
    "        gfp_single_list = sorted_nicely(glob.glob(os.path.join(gfp_single,\"*.tif\")))\n",
    "        df2 = df_format(gfp,gfp_list,df)\n",
    "        gfp_stack_list = df2[0]\n",
    "        n = df2[1]\n",
    "        df2 = df2[2]\n",
    "        \n",
    "        csv_format(df2.loc[:,::2],gfp_stack_list,n,fluo_directory,r'PctPositive_GFP_MK.csv');\n",
    "        csv_format(df2.loc[:,1::2],gfp_stack_list,n,fluo_directory,r'PctPositive_GFP_Pplt.csv');\n",
    "    \n",
    "    else:\n",
    "        pass;\n",
    "else:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Fluo Graphs\n",
    "\n",
    "- Graphs are generated (w/ mean & std_dev) automatically, plotting %positive MKs vs Pplts --> Currently using to measure transfection efficiency. Use the formatted csvs for more specific plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fluo_plot(p1,p2,folder,numscans,well_list,ylabel): #mkpctpos,ppltpctpos\n",
    "    \n",
    "    mean_df1 = p1.groupby(np.arange(len(p1.columns))//numscans, axis=1).mean()\n",
    "    mean_df1.columns = well_list\n",
    "    sd_df1 = p1.groupby(np.arange(len(p1.columns))//numscans, axis=1).std()\n",
    "    sd_df1.columns = well_list\n",
    "\n",
    "    mean_df2 = p2.groupby(np.arange(len(p2.columns))//numscans, axis=1).mean()\n",
    "    mean_df2.columns = well_list\n",
    "    sd_df2 = p2.groupby(np.arange(len(p2.columns))//numscans, axis=1).std()\n",
    "    sd_df2.columns = well_list\n",
    "\n",
    "    c = 0\n",
    "    for column in mean_df1,sd_df1,mean_df2,sd_df2:\n",
    "        while c in range(len(well_list)):\n",
    "            fig1, ax1 = plt.subplots()\n",
    "            ax1.set_title(well_list[c])\n",
    "            ax1.set_xlabel(\"Hour\")\n",
    "            ax1.set_ylabel(ylabel)\n",
    "            mean_df_final = mean_df1[well_list[c]]\n",
    "            sd_df_final = sd_df1[well_list[c]]\n",
    "            mean_df_final2 = mean_df2[well_list[c]]\n",
    "            sd_df_final2 = sd_df2[well_list[c]]\n",
    "            mean_df_final.plot.line(yerr=sd_df_final,label='MKs')\n",
    "            mean_df_final2.plot.line(yerr=sd_df_final2,label='Pplts')\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "            plt.savefig(flgraph_directory + '\\\\' + well_list[c] + '.png', bbox_inches='tight')\n",
    "            plt.close()\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if fluo is not False:\n",
    "    \n",
    "    # purely gfp or rfp, not both yet\n",
    "    os.makedirs(os.path.join(fluo_directory, \"graphs\"), exist_ok=True)\n",
    "    flgraph_directory = os.path.join(fluo_directory, \"graphs\")\n",
    "    \n",
    "    if fluo == 1:\n",
    "        pass;\n",
    "    \n",
    "    elif fluo == 2:\n",
    "        p1 = df2.loc[:,::2]\n",
    "        p2 = df2.loc[:,1::2]\n",
    "        fluo_plot(p1,p2,flgraph_directory,numscans,well_list,r\"Percent RFP-Positive\")\n",
    "        \n",
    "    elif fluo == 3:\n",
    "        p1 = df2.loc[:,::2]\n",
    "        p2 = df2.loc[:,1::2]\n",
    "        fluo_plot(p1,p2,flgraph_directory,numscans,well_list,r\"Percent GFP-Positive\")    \n",
    "else:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skelProcessing\n",
    "\n",
    "- If no fluorescent images were detected in the image directory, the phase_skeleton pipeline will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_sk_pipeline = path_to_skel_pipeline_mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(image_directory, \"skeleton\"), exist_ok=True)\n",
    "skeleton_directory = os.path.join(image_directory,\"skeleton\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdir_pplt_label = os.path.join(output_directory,\"proplatelet_labels\");\n",
    "pplt_list = sorted_nicely(glob.glob(os.path.join(imdir_pplt_label,\"*.tiff\")));\n",
    "skel_list = single_list + pplt_list\n",
    "with open(os.path.join(image_directory,\"skel_list.txt\"), 'w') as f:\n",
    "    for item in skel_list:\n",
    "        f.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Skeleton Pipeline\n",
    "Use subprocess to run CellProfiler on the images/labels to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command = (path_to_cellprofiler,\"--run-headless\",\"--pipeline={}\".format(path_to_sk_pipeline),\n",
    "           \"--file-list={}\".format(os.path.join(image_directory,\"skel_list.txt\")),\n",
    "           \"--image-directory={}\".format(image_directory),\"--output-directory={}\".format(skeleton_directory))\n",
    "\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    \n",
    "out, err = process.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse csvs for network analysis \n",
    "Proplatelet structures are measured from the following csvs generated by the Skeleton pipeline.\n",
    "\n",
    "    1 results_MaskedProtrusions\n",
    "    2 results_prot_seed (pplt seed points)\n",
    "    3 results_ppltobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(skeleton_directory, \"raw_files\"), exist_ok=True)\n",
    "dst = os.path.join(skeleton_directory, \"raw_files\")\n",
    "for filename in os.listdir(skeleton_directory):\n",
    "    if filename.startswith('results'):\n",
    "        src = skeleton_directory + \"\\\\\" + filename\n",
    "        newdst = dst + '\\\\' + filename\n",
    "        move(src, newdst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skcsv(dst,csv,header,columns):\n",
    "    csv = open(os.path.join(dst,csv));\n",
    "    df = pd.read_csv(csv, usecols = header, index_col = False);\n",
    "    df.columns = columns;\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mp_head = [\"Location_Center_X\",\"Location_Center_Y\",\"Parent_ppltobj\"]; #MaskedProtrusions; \"Parent_protrusions\"\n",
    "mp_columns = [\"Masked_Protrusion_X\",\"Masked_Protrusion_Y\",\"Parent_ppltobj\"];\n",
    "mp_df = skcsv(dst,'results_MaskedProtrusions.csv',mp_head,mp_columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps_head = ['ImageNumber', 'ObjectNumber', 'Metadata_time', 'Location_Center_X', #protrusion_seeds\n",
    "       'Location_Center_Y',\n",
    "       'ObjectSkeleton_NumberBranchEnds_MaskedProtrusionsImage',\n",
    "       'ObjectSkeleton_NumberNonTrunkBranches_MaskedProtrusionsImage',\n",
    "       'ObjectSkeleton_NumberTrunks_MaskedProtrusionsImage',\n",
    "       'ObjectSkeleton_TotalObjectSkeletonLength_MaskedProtrusionsImage'];\n",
    "\n",
    "ps_columns = ['ImageNumber', 'ObjectNumber', 'Timepoint',\n",
    "              'ProtrusionSeed_X', 'ProtrusionSeed_Y',\n",
    "              'NumEndpointsBlue', 'NumBranchGreen', 'NumTrunksRed',\n",
    "              'TotalObjLength'];\n",
    "\n",
    "ps_df = skcsv(dst,'results_prot_seed.csv',ps_head,ps_columns);\n",
    "\n",
    "def i(x):\n",
    "    return (x[5] + x[6] + x[7]);\n",
    "\n",
    "x = ps_df.apply(i,axis=1);\n",
    "ps_df['TotalNodes'] = x;\n",
    "ps_df = ps_df.add(mp_df,axis='columns',fill_value=0);\n",
    "ps_df.to_csv(os.path.join(skeleton_directory,'Raw_Skel.csv'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "po_head = ['ImageNumber', 'ObjectNumber', 'Metadata_time',     #ppltobj\n",
    "       'Children_MaskedProtrusions_Count', 'Location_Center_X',\n",
    "       'Location_Center_Y', 'Mean_MaskedProtrusions_Location_Center_X',\n",
    "       'Mean_MaskedProtrusions_Location_Center_Y',\n",
    "       'Mean_MaskedProtrusions_Number_Object_Number'];\n",
    "\n",
    "po_columns = ['ImageNumber', 'ObjectNumber', 'Timepoint', \n",
    "              'MaskedProtrusionCount', 'PpltObj_X', 'PpltObj_Y', \n",
    "              'MeanPpltObj_X', 'MeanPpltObj_Y', \n",
    "              'MeanMaskedProtrusionsNumber'];\n",
    "\n",
    "po_df = skcsv(dst,'results_ppltobj.csv',po_head,po_columns);\n",
    "po_df.to_csv(os.path.join(skeleton_directory,'PpltObj.csv'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetworkX\n",
    "\n",
    "This python program is used to construct graphs to model proplatelet structures from the edges & vertices csvs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The edge list is a simple data structure that you'll use to create the graph. Each row represents a single edge of the graph with some edge attributes.\n",
    "- Node lists are usually optional in networkx and other graph libraries when edge lists are provided because the node names are provided in the edge list's first two columns. However, in this case, there are some node attributes that we'd like to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vertices_csv = open(os.path.join(skeleton_directory,r'vertices.csv'));\n",
    "v_df = pd.read_csv(vertices_csv);\n",
    "v_df.columns = ['image_number', 'vertex_number','y','x','labels','kind'];#rename i,j to y,x\n",
    "v_df = v_df[['vertex_number', 'x','y','labels', 'kind','image_number']];#swap y,x to x,y in vertices_csv\n",
    "v_df.columns = ['Node','x','y','Labels','Kind','ImageNumber'];#rename i,j to y,x\n",
    "v_df.to_csv(os.path.join(skeleton_directory,'Vert.csv')); #nodelist (optional w/ edgelist, adds more hashable attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges_csv = open(os.path.join(skeleton_directory,r'edges.csv'));\n",
    "header = ['image_number', 'v1', 'v2', 'length'];\n",
    "e_df = pd.read_csv(edges_csv, usecols = header, index_col = False);\n",
    "e_df.columns = ['ImageNumber', 'Node_1', 'Node_2', 'Distance'];\n",
    "e_df = e_df[['Node_1','Node_2','Distance','ImageNumber']];\n",
    "e_df.to_csv(os.path.join(skeleton_directory,'Edge.csv')); #edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im=1\n",
    "obj=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imslc(df,im): #edgelist/nodelist df,imagenum; slice edgelist by imagenumber\n",
    "    df = df[df[\"ImageNumber\"]==im]\n",
    "#     del df['ImageNumber']\n",
    "    return df;\n",
    "\n",
    "edgelist = imslc(e_df,im)\n",
    "nodelist = imslc(v_df,im) #Use these lists for all ppltobjs in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelList(df,im,obj): #df=ps_df,imnum,objnum\n",
    "    df1 = df[df[\"ImageNumber\"]==im]\n",
    "    df2 = df1[df1['Parent_ppltobj']==obj]\n",
    "    return list(df2[\"ObjectNumber\"].unique());\n",
    "\n",
    "l = labelList(ps_df,im,obj);\n",
    "a = [int(i) for i in l];\n",
    "a[:] = [x - 1 for x in a];\n",
    "nodelist = nodelist[nodelist.Labels.isin(a)];\n",
    "b = list(nodelist[\"Node\"]);\n",
    "# b[:] = [x - 1 for x in b]\n",
    "edgelist = edgelist[edgelist.Node_2.isin(b)]; #Slice the edgelist further by label -> 1 ppltobj per graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skelPlot(edge,node): #edgelist,nodelist\n",
    "#     g = nx.DiGraph();\n",
    "    g = nx.Graph();\n",
    "    \n",
    "    # Add edges and attributes\n",
    "    for i, elrow in edge.iterrows():\n",
    "        g.add_edge(elrow[0], elrow[1], attr_dict=elrow[2:].to_dict())\n",
    "    \n",
    "    # Add node attributes\n",
    "    for i, nlrow in nodelist.iterrows():\n",
    "        try:\n",
    "            g.node[nlrow['Node']].update(nlrow[1:].to_dict())\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    # Define node positions dict for plotting    \n",
    "    node_positions = {node[0]: (node[1]['x'], -node[1]['y']) for node in g.nodes(data=True)}\n",
    "    \n",
    "    color_map = []\n",
    "    for node in g:\n",
    "        if g.node[node]['Kind'] == 'E': #Endpoints changed from blue to red\n",
    "            color_map.append('red')\n",
    "#         elif g.node[node]['Kind'] == 'B': \n",
    "#             color_map.append('green')\n",
    "        else:\n",
    "            color_map.append('black') #Make all nodes except endpoints black\n",
    "                \n",
    "    plt.figure(figsize=(8, 12))\n",
    "    nx.draw(g, pos=node_positions,node_size=7, node_color = color_map)#,with_labels = True)\n",
    "    plt.title(\"Test\", size=15)\n",
    "    plt.show()\n",
    "    return g;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = skelPlot(edgelist,nodelist); #Are trunk connection distances=3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spl=nx.all_pairs_dijkstra_path_length(g) \n",
    "for n in spl:\n",
    "     print(n[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp=networkx.all_pairs_dijkstra_path(g) \n",
    "for n in sp:\n",
    "     print(n[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = input('First Node: ')\n",
    "y = input('Second Node: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = nx.all_shortest_paths(g, x, y)\n",
    "\n",
    "for path in paths:\n",
    "    total_length = 0\n",
    "    for i in range(len(path)-1):\n",
    "        source, target = path[i], path[i+1]\n",
    "        edge = g[source][target]\n",
    "        length = edge['length']\n",
    "        total_length += length\n",
    "    print('{}: {}'.format(path, total_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spl=nx.all_pairs_dijkstra_path_length(g)\n",
    "for n in spl:\n",
    "     print(n[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "po_csv = open(os.path.join(skeleton_directory,r'PpltObj.csv'));\n",
    "header = [\"ImageNumber\",'ObjectNumber', \"PpltObj_X\", \"PpltObj_Y\"];\n",
    "po_df = pandas.read_csv(po_csv, usecols = header, index_col = False);\n",
    "\n",
    "def centroid(po,i): #ppltobj csv,image #\n",
    "    x = po[po[\"ImageNumber\"] == i]; #slice ppltobj.csv by ImageNumber\n",
    "    del x[\"ImageNumber\"];\n",
    "    x = x.set_index('ObjectNumber').T.to_dict('list')\n",
    "    return x;\n",
    "\n",
    "centroid(po_df,6) #x[1] x[1][0] ->dict slicing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
